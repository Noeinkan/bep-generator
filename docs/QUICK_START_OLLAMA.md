# Quick Start Guide - Ollama Backend per BEP Generator

## Panoramica Rapida

Questo setup usa **Ollama** invece di PyTorch/Hugging Face per l'AI:

âœ… **Pro:**
- Setup in 10 minuti invece di 30-60 minuti
- Nessun training necessario
- QualitÃ  del testo superiore
- PiÃ¹ veloce (2-4 secondi vs 5-10 secondi)
- Meno RAM richiesta (8GB vs 16GB+)
- Setup piÃ¹ semplice

âŒ **Contro:**
- Richiede download di ~6GB una volta sola
- Richiede Ollama in esecuzione in background

---

## Setup Completo (10 minuti)

### Step 1: Installa Ollama (3 minuti)

#### Windows
```bash
# Scarica da: https://ollama.com/download/windows
# Oppure usa winget:
winget install Ollama.Ollama
```

#### Linux/Mac
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Verifica installazione:**
```bash
ollama --version
```

### Step 2: Scarica il Modello (5 minuti)

```bash
# Modello raccomandato (6GB)
ollama pull llama3.2:3b
```

**Output atteso:**
```
pulling manifest
pulling 6a0746a1ec1a... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 2.0 GB
...
success
```

**Alternative leggere:**
```bash
# Se hai hardware limitato (2GB)
ollama pull llama3.2:1b

# Per qualitÃ  superiore (4GB, richiede GPU)
ollama pull mistral:7b
```

### Step 3: Verifica Ollama (1 minuto)

```bash
# Test veloce
ollama run llama3.2:3b "Write a BIM project summary"

# Test completo con lo script
cd bep-generator
npm run verify:ollama
```

**Output atteso:**
```
âœ… Ollama Ã¨ in esecuzione su http://localhost:11434
âœ… Modello raccomandato 'llama3.2:3b' Ã¨ installato
âœ… Generazione completata in 2.45 secondi
ğŸ‰ TUTTI I TEST SUPERATI!
```

### Step 4: Avvia il BEP Generator (1 minuto)

```bash
# Dalla root del progetto
npm start
```

Questo avvierÃ :
- âœ… Frontend React â†’ http://localhost:3000
- âœ… Backend Node.js â†’ http://localhost:5001
- âœ… ML Service (Ollama) â†’ http://localhost:5003

---

## Test dell'Integrazione

### Test 1: API Health Check

```bash
curl http://localhost:5003/health
```

**Output atteso:**
```json
{
  "status": "healthy",
  "ollama_connected": true,
  "model": "llama3.2:3b",
  "backend": "Ollama"
}
```

### Test 2: Generazione Testo

```bash
curl -X POST http://localhost:5003/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "This project aims to",
    "field_type": "executiveSummary",
    "max_length": 150
  }'
```

**Output atteso:**
```json
{
  "text": "establish a comprehensive framework for Building Information Modeling implementation across all project phases...",
  "prompt_used": "This project aims to",
  "model": "llama3.2:3b"
}
```

### Test 3: Suggestion per Campo Specifico

```bash
curl -X POST http://localhost:5003/suggest \
  -H "Content-Type: application/json" \
  -d '{
    "field_type": "projectObjectives",
    "partial_text": "The main goals are",
    "max_length": 200
  }'
```

### Test 4: Frontend Integration

1. Apri http://localhost:3000
2. Crea un nuovo BEP
3. Vai in una sezione (es. Executive Summary)
4. Clicca sul pulsante AI "âœ¨ Generate"
5. Verifica che il testo venga generato

---

## Comandi Utili

### Package.json Scripts

```bash
# Start tutto (frontend + backend + ML)
npm start

# Start solo ML service
npm run start:ml

# Verifica Ollama
npm run verify:ollama

# Start con vecchio sistema PyTorch (se necessario)
npm run start:ml:old
```

### Ollama Commands

```bash
# Lista modelli installati
ollama list

# Info su un modello
ollama show llama3.2:3b

# Rimuovi un modello
ollama rm llama3.2:3b

# Aggiorna un modello
ollama pull llama3.2:3b

# Test interattivo
ollama run llama3.2:3b
```

### ML Service Diretto

```bash
# Start manualmente (Windows)
cd ml-service
start_ollama_service.bat

# Start manualmente (Linux/Mac)
cd ml-service
source venv/bin/activate
python api_ollama.py
```

---

## Troubleshooting

### âŒ "Cannot connect to Ollama"

**Problema:** Il ML service non riesce a connettersi a Ollama

**Soluzione:**
```bash
# Verifica che Ollama sia in esecuzione
curl http://localhost:11434/api/tags

# Se non risponde, avvia Ollama:
# Windows: Cerca "Ollama" nel menu Start
# Linux/Mac: ollama serve
```

### âŒ "Model not found"

**Problema:** Il modello non Ã¨ scaricato

**Soluzione:**
```bash
# Verifica modelli installati
ollama list

# Scarica il modello mancante
ollama pull llama3.2:3b
```

### âŒ Generazione molto lenta (>30 secondi)

**Problema:** Hardware insufficiente per il modello

**Soluzioni:**
1. Usa un modello piÃ¹ leggero:
```bash
ollama pull llama3.2:1b
```

2. Modifica `ml-service/api_ollama.py` e cambia il modello:
```python
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'llama3.2:1b')  # Cambia da 3b a 1b
```

3. Chiudi applicazioni non necessarie per liberare RAM

### âŒ "Port 5003 already in use"

**Problema:** Porta ML service occupata

**Soluzione:**
```bash
# Windows
netstat -ano | findstr :5003
taskkill /PID <PID> /F

# Linux/Mac
lsof -ti:5003 | xargs kill -9
```

### âŒ Testo generato di bassa qualitÃ 

**Soluzioni:**

1. Usa il modello 3B o piÃ¹ grande:
```bash
ollama pull llama3.2:3b
# oppure
ollama pull mistral:7b
```

2. Modifica temperatura in `api_ollama.py`:
```python
# Per testo piÃ¹ coerente (meno creativo)
temperature=0.3

# Per testo piÃ¹ creativo (meno coerente)
temperature=0.9
```

---

## Confronto Performance

### Ollama (llama3.2:3b) vs PyTorch LSTM

| Metrica | Ollama | PyTorch LSTM |
|---------|--------|--------------|
| Setup Time | 10 min | 30-60 min |
| Model Download | 6 GB | Training dataset |
| Training Required | âŒ No | âœ… Yes (15-30 min) |
| RAM Required | 8 GB | 16+ GB |
| GPU Required | âš ï¸ Optional | âš ï¸ Recommended |
| Generation Speed | 2-4 sec | 5-10 sec |
| Text Quality | â­â­â­â­â­ | â­â­â­â­â˜† |
| Context Understanding | â­â­â­â­â­ | â­â­â­â˜†â˜† |
| Ease of Setup | â­â­â­â­â­ | â­â­â˜†â˜†â˜† |

---

## Architettura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     BEP Generator                           â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Frontend   â”‚    â”‚   Backend    â”‚    â”‚  ML Service  â”‚ â”‚
â”‚  â”‚  React:3000  â”‚â—„â”€â”€â–ºâ”‚  Node:5001   â”‚â—„â”€â”€â–ºâ”‚ Python:5003  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                   â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚
                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                                           â”‚     Ollama      â”‚
                                           â”‚  localhost:11434â”‚
                                           â”‚                 â”‚
                                           â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                                           â”‚ â”‚llama3.2:3b  â”‚ â”‚
                                           â”‚ â”‚   (6GB)     â”‚ â”‚
                                           â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flusso di Generazione Testo:

1. **User** digita testo nel frontend â†’ clicca "Generate"
2. **Frontend** invia richiesta POST a `/api/ml/suggest`
3. **Backend** forward a ML Service `/suggest`
4. **ML Service** chiama Ollama API `/api/generate`
5. **Ollama** genera testo con LLM locale
6. **Risposta** ritorna attraverso la catena
7. **Frontend** mostra il testo generato

---

## File Importanti

### Nuovi File Ollama
- `ml-service/api_ollama.py` - FastAPI service con Ollama backend
- `ml-service/ollama_generator.py` - Generatore testo con Ollama
- `ml-service/verify_ollama.py` - Script verifica setup
- `ml-service/start_ollama_service.bat` - Avvio rapido Windows
- `docs/OLLAMA_SETUP.md` - Documentazione completa setup

### File Esistenti (backup)
- `ml-service/api.py` - Vecchia API con PyTorch (ancora funzionante)
- `ml-service/model_loader.py` - Vecchio loader LSTM (ancora funzionante)

### Configurazione
- `package.json` - Aggiornato con `start:ml` â†’ Ollama
- `package.json` - `start:ml:old` â†’ PyTorch (fallback)

---

## Prossimi Passi

Dopo aver completato il setup:

1. âœ… Verifica che tutto funzioni: `npm run verify:ollama`
2. âœ… Avvia il sistema: `npm start`
3. âœ… Testa la generazione nel frontend
4. ğŸ“– Leggi [OLLAMA_SETUP.md](./OLLAMA_SETUP.md) per dettagli avanzati
5. ğŸ¯ Sperimenta con diversi modelli e temperature
6. ğŸš€ Deploy in produzione (opzionale)

---

## FAQ

### 1. Posso usare entrambi i sistemi (Ollama e PyTorch)?

SÃ¬, puoi passare da uno all'altro:
```bash
# Ollama (default)
npm run start:ml

# PyTorch (vecchio)
npm run start:ml:old
```

### 2. Quale modello Ã¨ meglio per il mio hardware?

| Hardware | Modello Raccomandato | RAM | Download |
|----------|---------------------|-----|----------|
| GPU 8GB+ | llama3.2:3b | 8GB | 6GB |
| GPU 4GB+ | llama3.2:1b | 4GB | 2GB |
| CPU only | llama3.2:1b | 8GB | 2GB |
| Potente  | mistral:7b | 16GB | 4GB |

### 3. Ollama funziona offline?

SÃ¬! Dopo aver scaricato il modello, Ollama funziona completamente offline.

### 4. Come cambio modello?

```bash
# Opzione 1: Variabile ambiente
set OLLAMA_MODEL=llama3.2:1b
npm run start:ml

# Opzione 2: Modifica api_ollama.py
# Cambia la riga: OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'llama3.2:3b')
```

### 5. Ollama usa la mia GPU?

SÃ¬, Ollama rileva automaticamente la GPU NVIDIA con CUDA e la usa per accelerazione.

---

## Supporto

- **Issue GitHub**: [bep-generator/issues](https://github.com/yourusername/bep-generator/issues)
- **Ollama Docs**: https://github.com/ollama/ollama/blob/main/docs/api.md
- **Discord Ollama**: https://discord.gg/ollama

---

**ğŸ‰ Buon lavoro con il tuo BEP Generator potenziato da AI!**
